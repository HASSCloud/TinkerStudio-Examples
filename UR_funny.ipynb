{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly we need to install the Torch package as this is not in the built in suite\n",
    "\n",
    "this code works on pickle - and PKL file types\n",
    "\n",
    "based upon this code https://github.com/ROC-HCI/UR-FUNNY/blob/master/humor_dataloader.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/envs/py36/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py36/lib/python3.6/site-packages (from torch) (1.15.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_pickle(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print('Unable to load data ', pickle_file, ':', e)\n",
    "        raise\n",
    "    return pickle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, id_list,max_context_len=5,max_sen_len=20):\n",
    "        self.id_list = id_list\n",
    "        openface_file=\"openface_features_sdk.pkl\"\n",
    "        covarep_file=\"covarep_features_sdk.pkl\"\n",
    "        word_idx_file=\"word_embedding_indexes_sdk.pkl\"\n",
    "        word_embedding_list_file=\"word_embedding_list.pkl\"\n",
    "        humor_label_file=\"humor_label_sdk.pkl\"\n",
    "        \n",
    "        self.word_aligned_openface_sdk=load_pickle(openface_file)\n",
    "        self.word_aligned_covarep_sdk=load_pickle(covarep_file)\n",
    "        self.word_embedding_idx_sdk=load_pickle(word_idx_file)\n",
    "        self.word_embedding_list_sdk=load_pickle(word_embedding_list_file)\n",
    "        self.humor_label_sdk = load_pickle(humor_label_file)\n",
    "        self.of_d=75\n",
    "        self.cvp_d=81\n",
    "        self.max_context_len=max_context_len\n",
    "        self.max_sen_len=max_sen_len\n",
    "    \n",
    "    #left padding with zero  vector upto maximum number of words in a sentence * glove embedding dimension \n",
    "    def paded_word_idx(self,seq,max_sen_len=20,left_pad=1):\n",
    "        seq=seq[0:max_sen_len]\n",
    "        pad_w=np.concatenate((np.zeros(max_sen_len-len(seq)),seq),axis=0)\n",
    "        pad_w=np.array([self.word_embedding_list_sdk[int(w_id)] for  w_id in pad_w])\n",
    "        return pad_w\n",
    "    \n",
    "    #left padding with zero  vector upto maximum number of words in a sentence * covarep dimension \n",
    "    def padded_covarep_features(self,seq,max_sen_len=20,left_pad=1):\n",
    "        seq=seq[0:max_sen_len]\n",
    "        return np.concatenate((np.zeros((max_sen_len-len(seq),self.cvp_d)),seq),axis=0)\n",
    "    \n",
    "    #left padding with zero  vector upto maximum number of words in a sentence * openface dimension \n",
    "    def padded_openface_features(self,seq,max_sen_len=20,left_pad=1):\n",
    "        seq=seq[0:max_sen_len]\n",
    "        return np.concatenate((np.zeros(((max_sen_len-len(seq)),self.of_d)),seq),axis=0)\n",
    "    \n",
    "    #left padding with zero vectors upto maximum number of sentences in context * maximum num of words in a sentence * 456\n",
    "    def padded_context_features(self,context_w,context_of,context_cvp,max_context_len=5,max_sen_len=20):\n",
    "        context_w=context_w[-max_context_len:]\n",
    "        context_of=context_of[-max_context_len:]\n",
    "        context_cvp=context_cvp[-max_context_len:]\n",
    "\n",
    "        padded_context=[]\n",
    "        for i in range(len(context_w)):\n",
    "            p_seq_w=self.paded_word_idx(context_w[i],max_sen_len)\n",
    "            p_seq_cvp=self.padded_covarep_features(context_cvp[i],max_sen_len)\n",
    "            p_seq_of=self. padded_openface_features(context_of[i],max_sen_len)\n",
    "            padded_context.append(np.concatenate((p_seq_w,p_seq_cvp,p_seq_of),axis=1))\n",
    "\n",
    "        pad_c_len=max_context_len-len(padded_context)\n",
    "        padded_context=np.array(padded_context)\n",
    "        \n",
    "        #if there is no context\n",
    "        if not padded_context.any():\n",
    "            return np.zeros((max_context_len,max_sen_len,456))\n",
    "        \n",
    "        return np.concatenate((np.zeros((pad_c_len,max_sen_len,456)),padded_context),axis=0)\n",
    "    \n",
    "    def padded_punchline_features(self,punchline_w,punchline_of,punchline_cvp,max_sen_len=20,left_pad=1):\n",
    "        \n",
    "        p_seq_w=self.paded_word_idx(punchline_w,max_sen_len)\n",
    "        p_seq_cvp=self.padded_covarep_features(punchline_cvp,max_sen_len)\n",
    "        p_seq_of=self.padded_openface_features(punchline_of,max_sen_len)\n",
    "        return np.concatenate((p_seq_w,p_seq_cvp,p_seq_of),axis=1)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id_list)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        hid=self.id_list[index]\n",
    "        punchline_w=np.array(self.word_embedding_idx_sdk[hid]['punchline_embedding_indexes'])\n",
    "        punchline_of=np.array(self.word_aligned_openface_sdk[hid]['punchline_features'])\n",
    "        punchline_cvp=np.array(self.word_aligned_covarep_sdk[hid]['punchline_features'])\n",
    "        \n",
    "        context_w=np.array(self.word_embedding_idx_sdk[hid]['context_embedding_indexes'])\n",
    "        context_of=np.array(self.word_aligned_openface_sdk[hid]['context_features'])\n",
    "        context_cvp=np.array(self.word_aligned_covarep_sdk[hid]['context_features'])\n",
    "        \n",
    "        #punchline feature\n",
    "        x_p=torch.LongTensor(self.padded_punchline_features(punchline_w,punchline_of,punchline_cvp,self.max_sen_len))\n",
    "        #context feature\n",
    "        x_c=torch.LongTensor(self.padded_context_features(context_w,context_of,context_cvp,self.max_context_len,self.max_sen_len))\n",
    "        \n",
    "        y=torch.FloatTensor([self.humor_label_sdk[hid]])\n",
    "                \n",
    "        return x_p,x_c,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folds_file=\"data_folds.pkl\"\n",
    "data_folds=load_pickle(data_folds_file)\n",
    "train=data_folds['train']\n",
    "dev=data_folds['dev']\n",
    "test=data_folds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16514"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)+len(dev)+len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next section will not work untill the pkl files are fully loaded into the workspace,  the covarep_features and openface_feaures files are huge and need time to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set = HumorDataset(train)\n",
    "dev_set = HumorDataset(dev)\n",
    "test_set = HumorDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=10\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_set, batch_size=batch, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********\n",
      "punchline shape:  torch.Size([10, 20, 456])\n",
      "context shape:  torch.Size([10, 5, 20, 456])\n",
      "humor labels:  tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "*********\n",
      "punchline shape:  torch.Size([10, 20, 456])\n",
      "context shape:  torch.Size([10, 5, 20, 456])\n",
      "humor labels:  tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "*********\n",
      "punchline shape:  torch.Size([10, 20, 456])\n",
      "context shape:  torch.Size([10, 5, 20, 456])\n",
      "humor labels:  tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "*********\n",
      "punchline shape:  torch.Size([10, 20, 456])\n",
      "context shape:  torch.Size([10, 5, 20, 456])\n",
      "humor labels:  tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "*********\n",
      "punchline shape:  torch.Size([10, 20, 456])\n",
      "context shape:  torch.Size([10, 5, 20, 456])\n",
      "humor labels:  tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "*********\n",
      "punchline shape:  torch.Size([10, 20, 456])\n",
      "context shape:  torch.Size([10, 5, 20, 456])\n",
      "humor labels:  tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for  batch_idx, batch in enumerate(train_dataloader, 0): \n",
    "    x_p,x_c,y=map(lambda x: x.to(device), batch)\n",
    "    print(\"*********\")\n",
    "    print(\"punchline shape: \",x_p.shape)\n",
    "    print(\"context shape: \",x_c.shape)\n",
    "    print(\"humor labels: \",y)\n",
    "    if batch_idx==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
