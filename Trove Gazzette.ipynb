{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Trove Government Gazettes\n",
    "\n",
    "Attempting to reproduce the work described [on the NLA blog](https://www.nla.gov.au/blogs/trove/2018/07/23/digital-tools-for-big-research) where a collection of Certificates of Naturalisation were selected from the Trove Government Gazettes and analysed to give a picture of the number of arrivals over time. \n",
    "\n",
    "In that exercise the work was done manually to identify names and generate counts.  I will attempt to implement an automated process to derive the same data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, please ensure that you have a `secret.json` file in the current working directory (generally this is your workspace.)<br />If you haven't got this file, run the ***Set up secrets*** notebook first, then return here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first\n",
    "!curl -s -O -L https://raw.githubusercontent.com/HASSCloud/TinkerStudio-Examples/master/{requirements.txt,utils.py}\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import datetime\n",
    "import utils\n",
    "TROVE_API_KEY = utils.secret('trove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trove_query(q, n=100):\n",
    "    \"\"\"A simple Trove API interface, \n",
    "    q is a query term, we search the \n",
    "    newspaper zone and return\n",
    "    the decoded JSON response (a Python dictionary)\"\"\"\n",
    "    \n",
    "    TROVE_API_URL = \"http://api.trove.nla.gov.au/result\"\n",
    "    qterms = {\n",
    "        'zone': 'newspaper',\n",
    "        'encoding': 'json',\n",
    "        'include': 'articleText',\n",
    "        's': 0,\n",
    "        'n': n,\n",
    "        'key': TROVE_API_KEY,\n",
    "        'q': q\n",
    "    }\n",
    "    r = requests.get(TROVE_API_URL, params=qterms).json()\n",
    "    articles = r['response']['zone'][0]['records']['article']\n",
    "    remaining = n-100\n",
    "    while remaining > 0:\n",
    "        qterms['n'] = remaining\n",
    "        qterms['s'] += 100\n",
    "        r = requests.get(TROVE_API_URL, params=qterms)\n",
    "        r = r.json()\n",
    "        art = r['response']['zone'][0]['records']['article']\n",
    "        if len(art) > 0:\n",
    "            articles.extend(art)\n",
    "            remaining -= 100\n",
    "        else:\n",
    "            # no more articles\n",
    "            remaining = 0\n",
    "        \n",
    "    return articles\n",
    "\n",
    "#articles = trove_query('\"Certificate of Naturalisation\"', 110)\n",
    "#len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = trove_query('\"Certificates of Naturalisation\"', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(articles[0]['articleText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lines = re.findall('<span>([^<]+)</span>', articles[5]['articleText'])\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Spacy NER\n",
    "\n",
    "We'll try to use NER on this text to find names. However, given the lack of context in the text (this is just a list of names) it may not be very successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from IPython.core.display import display, HTML\n",
    "# download the spacy models we need\n",
    "model = 'en_core_web_md'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the NER model and displaying the output for this text we see that while many  names are highlighted (in purple) there are also many missed and many false positives shown.   The lack of context in the text removes the usual cues to names and leaves the system guessing based on capitalisation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\\n\".join(lines))\n",
    "display(HTML(displacy.render(doc, style='ent')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression based Extraction\n",
    "\n",
    "In this case the text is very structured as a list of names, addresses and dates.  We can try to use regular expressions to locate these fields in the text.\n",
    "\n",
    "First find the lines in the text containg date-like words (digits + .)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = re.findall('<span>\\W*([^<]+)\\W*</span>', articles[5]['articleText'])\n",
    "print(lines[:10])\n",
    "datelines = [m for m in lines if re.search('\\d+', m)]\n",
    "datelines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join all of these lines together into one big string since records seem to flow over lines. \n",
    "\n",
    "We can then look for the individual records. Each record looks like:\n",
    "\n",
    "> Cianetti,  Carla,  68  West  Street,  Mt  Isa,  10.7.67.\n",
    "\n",
    "which we can generalise to:\n",
    "\n",
    "> Last, First, Address, Date\n",
    "\n",
    "So let's write a regular expression pattern to match that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(datelines)\n",
    "pattern = \"\\W+(.+?)(\\d\\d?[ .]+\\d\\d?[ .]+\\d\\d)[.;]?\"\n",
    "matches = re.findall(pattern, text)\n",
    "matches[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for text, date in matches:\n",
    "    n = text.split(',')\n",
    "    if len(n) > 2:\n",
    "        res.append({'first': n[1].strip(), 'last': n[0].strip(), 'addr': \" \".join(n[2:]).strip()})\n",
    "res[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all that into a function\n",
    "\n",
    "def extract_names(document):\n",
    "    \"\"\"Extract a list of names from a CERTIFICATION OF NATURALISATION \n",
    "    article in Trove Government Gazettes\"\"\"\n",
    "    \n",
    "    if 'articleText' in document:\n",
    "        lines = re.findall('<span>\\W*([^<]+)\\W*</span>', document['articleText'])\n",
    "        datelines = [m for m in lines if re.search('\\d+', m)]\n",
    "\n",
    "        text = \" \".join(datelines)\n",
    "        pattern = \"\\W+(.+?)(\\d\\d?)[ .]+(\\d\\d?)[ .]+(\\d\\d)[.;]?\"\n",
    "        matches = re.findall(pattern, text)\n",
    "\n",
    "        result = []\n",
    "        badlines = []\n",
    "        for text, day, month, year in matches:\n",
    "            n = text.split(',')\n",
    "            if len(n) > 2:\n",
    "                try:\n",
    "                    date = datetime.datetime(day=int(day), month=int(month), year=int(\"19\"+year))\n",
    "                    result.append({'article': document['url'],\n",
    "                               'first': n[1].strip(), \n",
    "                               'last': n[0].strip(), \n",
    "                               'addr': \" \".join(n[2:]).strip(),\n",
    "                               'date': date,\n",
    "                               'articledate': pd.to_datetime(document['date'])\n",
    "                              })\n",
    "                except ValueError:\n",
    "                    date = day + month + year\n",
    "                    badlines.append(document)\n",
    "                    \n",
    "        return result, badlines\n",
    "    else:\n",
    "        print(document.keys())\n",
    "        return [], []\n",
    "    \n",
    "#extract_names(articles[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_names(articles[0])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "bad = []\n",
    "counts = []\n",
    "for art in articles:\n",
    "    names, badlines = extract_names(art)\n",
    "    #print(\"^^-----\", art['heading'], art['url'], len(names), \"------^^\\n\")\n",
    "    result.extend(names)\n",
    "    bad.extend(badlines)\n",
    "    counts.append({'id': art['id'], 'date': art['date'], 'count': len(names), 'bad': len(badlines)})\n",
    "\n",
    "counts = pd.DataFrame(counts)\n",
    "print(\"Got error lines: \", len(bad))\n",
    "names = pd.DataFrame(result)\n",
    "print(\"Got \", names.shape[0], \"names\")\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.groupby('last').size().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names['ayear'] = [x.year for x in names['articledate']]\n",
    "names['month'] = [x.month for x in names['articledate']]\n",
    "by_year = names.groupby('articledate').size().sort_index()\n",
    "print(by_year.index.min(), by_year.index.max())\n",
    "plt.figure(figsize=(15,6))\n",
    "by_year.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.distplot(names['month'], rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts['count'] > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder \n",
    "GEONAMES_KEY = utils.secret('geonames')\n",
    "loc = names['addr'][5]\n",
    "g = geocoder.geonames(loc, key=GEONAMES_KEY, countryBias=['AU'])\n",
    "g.address, g.lat, g.lng, loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_KEY=utils.secret('google')\n",
    "\n",
    "for loc in names['addr'][10:20]:\n",
    "    g = geocoder.google(loc, key=GOOGLE_KEY)\n",
    "    print(g.address, g.lat, g.lng, loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
