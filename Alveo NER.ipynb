{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Workflow: Alveo\n",
    "\n",
    "This worksheet pulls data from the Alveo API and performs some NER using Spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, please ensure that you have a `secret.json` file in the current working directory (generally this is your workspace.)<br />If you haven't got this file, run the ***Set up secrets*** notebook first, then return here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first\n",
    "!curl -s -O -L https://raw.githubusercontent.com/HASSCloud/TinkerStudio-Examples/master/{requirements.txt,utils.py}\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install requests_toolbelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import geocoder\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "import pyalveo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alveo requires a login and uses an API key to validate user requests. We read this from the file `secret.json`.  \n",
    "\n",
    "The data we will work with is represented by an [item list](http://alveo.edu.au/documentation/discovering-and-searching-the-collections/saving-your-search-results-to-an-item-list/) in Alveo - this is a list of items selected via a query as the starting point for a research project.   In this case I've selected three items from the [Braided Channels](https://app.alveo.edu.au/catalog/braidedchannels) collection that contains transcripts of oral history interviews.  Each item list has a URL and we refer to that here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = utils.secret('alveo')\n",
    "API_URL = \"https://app.alveo.edu.au/\"\n",
    "#item_list_url = \"https://app.alveo.edu.au/item_lists/1387\"\n",
    "item_list_url = \"https://app.alveo.edu.au/item_lists/1172\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an API client with the pyalveo module and use the client to get the item list details.  We then get the _primary text_ for each item.  We store these in a python list of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client  =  pyalveo.Client(api_key=API_KEY, api_url=API_URL)\n",
    "itemlist = client.get_item_list(item_list_url)\n",
    "\n",
    "print(\"Item list name: \", itemlist.name())\n",
    "\n",
    "texts = []\n",
    "for itemurl in itemlist:\n",
    "    item = client.get_item(itemurl)\n",
    "    text = item.get_primary_text()\n",
    "    text = text.decode() # convert from bytes to a string\n",
    "    text = re.sub('\\W+', ' ', text)\n",
    "    texts.append(text) \n",
    "\n",
    "print(\"Got\", len(texts), \"texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Using Spacy\n",
    "\n",
    "We will use Spacy to extract Named Entities from the text.   We download the appropriate models and initialise an NLP processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the spacy models we need\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extract entities from the texts.  The results will be converted to a Pandas data frame. In this example we retain all of the entity types in the result and for each result include a _context_ string showing the words each side of the entity that was found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        context = doc[ent.start-2:ent.end+3]\n",
    "        context = \" \".join([w.text for w in context])\n",
    "        d = {'label': ent.label_, 'text': ent.text, 'context': context}\n",
    "        places.append(d)\n",
    "\n",
    "entities = pd.DataFrame(places)\n",
    "print(\"Found \", entities.shape[0], \"entities in the texts\")\n",
    "entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be particularly interested in the GPE entities - locations.  We can select these as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = entities[entities['label'] == 'GPE']\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the frequency of occurence of each place name in the texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "grouped = locations.groupby('text')\n",
    "counts = grouped.size()\n",
    "counts.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
